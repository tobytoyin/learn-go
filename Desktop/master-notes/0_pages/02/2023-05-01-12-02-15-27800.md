---
id: F257D331-CFFD-4B50-8FB2-D85CF054B7F6
creation-date: 2023-05-01T12:02:15
type: concept
aliases:
  - Micro-Partition
tags:
  - snowflake
  - data-storage
catalogs:
  - c-2023-05-01-11-40
---

*Micro-partitions* is the storage optimisation in [[0_pages/02/2023-02-26-22-05-18-97000|Snowflake]]. From a high-level, micro-partitioning is the hybrid of [[0_pages/05/2023-05-01-11-20-15-70300|row-based & column-based partitioning]]. 

---
## Architectural Overview

![[3_hidden/_images/Pasted image 20230501161820.png]]

1. micro-partitions are created by partitioning the full Table into 50-500 MBs rows-sized Tables (by row orders)
2. each micro-partitions are compressed by each column into columnar Tables
	- all Row data are preserved, unlike most columnar-files compression
	- e.g., if a Table has 2 columns, micro-partitions compressed the Table into two 2 columnar files for storage
4. metadata are created for each micro-partition, containing statistics of each columns. These are used for scanning & planning during query execution

> [!NOTE]- Snowflake's Physical Structure Illustration
> ![[3_hidden/_images/Pasted image 20230501155525.png]]

---
## Benefits 

### Reduce Skewed Datasets

By adding row-based partitioning to breaking down a large Table, this aims to reduce the problem of [[0_pages/03/2023-02-26-17-12-42-45600#Skewed|skewed datasets]]. This is because each partition won't be hot-keyed by the specified partitioning columns. 

### Efficient Bulk INSERT 

![[3_hidden/_images/Pasted image 20230501194825.png]]
Bulk data Insertion can be efficiently done using micro-partitions. New rows simply being added as new micro-partitions, which can reduce storage overheads for [[0_pages/05/2023-03-01-21-32-42-34200|partitions rebalancing]] in traditional partitioned storage.

### Efficient Bulk UPDATE

![[3_hidden/_images/Pasted image 20230501195104.png]]

Bulk data updates in micro-partitions is actually a "delete-insert" operations: 
- query engine scans for micro-partitions that needs to be "updated"
- data are updated by "overwriting" the partition that needs updating
- "outdated micro-partitions" are pruned from the storage; "updated micro-partitions" are added as a new partitions

This method is quite similar to how traditional database performs "UPDATE" by appending newer data. 


### Efficient Computation

![[3_hidden/_images/Pasted image 20230501161406.png]]

As Snowflake [[0_pages/02/2023-02-26-22-04-27-31600|separates storages & computation layers]], having micro-partitions can allow more distributed handling: 
- **reduce overheads in shuffling/ gathering data** into compute resources, micro-partitioned data can be independently handled by a single computing instance
- **more elasticity in computation** - when data in micro-partitions are not needed in the query, no computation resources would be needed. This reduces wasting un-utilised computation resources

---
## ‚ÑπÔ∏è¬† Resources
- [[üìï Snowflake The Definitive Guide#^chapter-9]]
- [Introduction to Snowflake's Micro-Partitions](https://select.dev/posts/introduction-to-snowflake-micro-partitions)
- [Micro-partitions & Data Clustering | Snowflake Documentation](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions)
- [[üìï Fundamental of Data Engineering#^chapter-6]]