---
creation-date: 2022-12-10T17:36:10
type: concept
aliases:
  - MapReduce
tags:
  - programming
catalogs:
  - 2022-12-10-17-36
---

# MapReduce Key Ideas 

MapReduce execution is similar to how [[0_pages/01/2023-03-11-15-34-12-98200|Unix Software]] handles inputs and outputs, both of which emphasise on handling fragment of inputs (*Records*) with simple functions, then gather the final results. This is a _split-apply-combine_ computing strategy.

A MapReduce job focuses on 3 phrases in handling [[0_pages/05/2023-02-26-15-35-58-33600|batch of files]]: 
- Files Parsing Stage
- Mapping Stage 
- Reduce Stage ^61a1d5

![[3_hidden/_images/map-reduce-20221210174754.png|700]]^3d30e2

---
## Files Parsing Stage

This stage is not part of the MapReduce job responsible but it is required to enable MapReduce. ==This is handled by other input files parser software which would break down input files into separate Records==. For example, a parser to would read in CSV file(s), then convert data rows into MapReduce Records.

---
## Mapping Stage

The key purpose of a Mapping Stage is to generate a key-value paired Record: `{key: func(Record)}`.

A Record's key is ==extracted/ assigned from the Record==. The key could be assigned as:

- extracting from the Record (e.g., some Enums)
- randomly (i.e., *Shuffling* through an range of random identifiers)

==A Record's value is generated by a "*Mapper Function*"==. A Mapper Function is a callback function that can be executed onto individual Record independently and return certain value from the function. 

Once these tasks are completed, Mapper would sort the outputs. Eventually, keys that have the same value would be sorted along with each others.

> [!note]- Stateless Mapper
> The Mapper Function is following a Functional Programming approach when applied to individual Records. The Output doesn't contain any state of the Input once the Function has completed execution.

---
## Reduce Stage

Reduce Stage starts by two tasks. It would ==collect all the Records that share the same key assigned by the Mapper==. This merge all the Records grouped by the same key into a batch/ collection. Then a "*Reducer Function*" would start ==computation iteratively on each Record of the batch== and writes the output Records.  ^ec02e6

> [!note]- Stateful Reducer
> By iteration, the Reducer would chain/ "pipe" its output to the next Record. This allow the Reducer to retain states of all Records it iterated through and generate results need to look through all the records, e.g., data aggregation/ searching. 
> 

---
## ℹ️ Resources
- [[📕 Design Data-Intensive Applications#^batch-process]]