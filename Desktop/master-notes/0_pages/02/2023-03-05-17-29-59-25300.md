---
id: 4F4D40AF-4819-43A2-9618-16D3B3F57D76
creation-date: 2022-12-26T14:15:21 
type: concept
alias: 
- Spark Application Lifecycles
- "Spark Engine"
- "Spark Workflow"
- "Jobs"
- "Tasks"
- Stages
tags: [technology/spark, programming ]
---

# Spark Application Lifecycles 


## Job

A *Spark Job* indicates one action in the application that return results back to the [[0_pages/04/2023-02-26-17-21-15-09800#^8776b7|Driver Process]] (usually during when `show`, `write`, `collect` is executed).

A single Job would contain multiple [[#Stage|Stages]] depending on how complex the overall script is. When a Spark Job is handling data with a lots of joins and need to bring data into the same partition, a Job would contain lots of Stages. 

![[1_catalog/14/2022-12-26-14-39#^spark-job]]

> [!note]- Similarity to MapReduce Workflow
> A Spark Job is similar to one [[0_pages/02/2023-02-26-17-18-32-70300#MapReduce Workflow|MapReduce Workflow]], where a Job completes multiple MapReduce processes and write out the results. The difference is that Spark as a *Dataflow Engine* doesn't need to write out every MapReduce results.


---
## Stage

A *Spark Stage* is a group that consists of multiple [[#Tasks]] running on a single [[0_pages/04/2023-02-26-17-21-15-09800|executor]]. Every time Spark needs to conduct a repartitioning/ shuffling of data (as [[0_pages/04/2023-02-26-17-20-55-53700|Partitioner]]) to other Executors, it finishes a Stage. 
In other word, all the computation that belongs to can be done within the original partition remains in a one Stage.

![[1_catalog/14/2022-12-26-14-39#^spark-stage]]

> [!note]- Similarity to MapReduce Shuffle
> In MapReduce, [[0_pages/02/2023-02-26-15-39-18-14900#üîÄ Shuffle Stage|shuffling]] is done after a mapping stage so that data can be similar can be redistributed for other tasks. 
> Although Spark follows a similar strategy to redistribute data into the same partitions for optimisation. 
> 
> ![[1_catalog/14/2022-12-26-14-39#^5bdd9f]]

---
## Tasks

![[1_catalog/14/2022-12-26-14-39#^spark-task]] ^852206

A *Spark Task* is one computation step that apply on an Executor's partition. Within a Spark Stage, partitions would consist of multiple partitioned data, each partition would apply one set of Spark Tasks and a single Stage would comprise of many tasks there are many partitions.

Usually, one basic `F.<func>` call on Spark indicates one Task; more complex tasks like aggregation usually involves more Tasks to compute different values for aggregating. 

Task can operates on the same partition and chain multiple Tasks continuously. The [[0_pages/03/2023-02-26-17-20-20-43600|Intermediate Results]] of each Task are stored "in-memory". Chain of Tasks ends when results need to be repartitioned, which ends one Spark Stage as well.

> [!note]- Similarity to MapReduce Mapping
> This is similar to one [[0_pages/02/2023-02-26-15-39-18-14900#üó∫Ô∏è Mapping Stage]] in a MapReduce.

---
## Scope in Spark Cluster

![[1_catalog/14/2022-12-26-14-39#^194089]]

Another way to look at Job, Stage, Task is by looking at the computation unit with respect to a [[0_pages/04/2023-02-26-17-21-15-09800|Spark Cluster]]: 
- Spark Job - one computation unit within a Spark Cluster
- Spark Stage - one computation unit within a cluster's Node (Executor)
- Spark Task - one computation unit within a Node's partition

---
## Dataflow vs MapReduce

Spark Application is a *Dataflow Engine* but most of the computation workflow is similar [[0_pages/02/2023-02-26-15-39-18-14900|MapReduce]]. While MapReduce generates intermediate outputs. Spark don't write intermediate outputs unless it is required to (i.e., [[0_pages/04/2023-02-26-21-03-31-88200|lazy evaluated until actions]]).

- Spark Job - similar to a one Job composes of multiple MapReduces
- Spark Stage - similar to one MapReduce operation
- Spark Task - similar to one MapReduce's Mapper Function

---
## ‚ÑπÔ∏è¬† Resources
- [[Apache Spark The Definitive Guide#^chapter-16]]