---
id: 2B68FCD4-81EF-48FE-936F-A3FB618B55F1
creation-date: 2023-01-10T20:41:15
type: concept
aliases:
  - Spark Performance Symtoms
tags:
  - technology/spark
  - optimisation
  - use-cases
catalogs:
---

# Spark Performance Symtoms 

A Spark Job usually have several types of *performance issue*: 

---
## Slow Tasks

*Slow Tasks* are common occur in three ways: 
- insufficient resources - each executors have insufficient in-memory to computation, thus lead to [[Spill]]
- [[0_pages/03/2023-02-26-17-12-42-45600|skewed]] partitions that is results from prior aggregations, i.e., Spark hasn't execute shuffle automatically
- large data lineage - RDDs contains too many transformation histories

### Symtoms
- Slow basics operations after some aggregations 

[[0_pages/03/2023-02-26-17-19-57-90800|Optimising Spark Partitions]]

---
## Slow Aggregation & Slow Joins

*Slow Aggregations/ Joins* raises when partitions are [[0_pages/03/2023-02-26-17-12-42-45600|skewed]]: 

![[0_pages/03/2023-02-26-17-19-37-01900#^skewed-partition]]

### Symtoms
- Adding more resources (i.e., Executors) cannot reduce processing time 
- [[0_pages/02/2023-03-11-15-38-01-59500#Executors|Executors metrics]] showing higher shuffle read/ write rate for certain Executors. This indicates some ==Executors are skewed and working on more data==. 
- *Multiple of slow Stages* indicates lots of shuffling needs to be done
- Stages before/ after Join statements are working fine 

---
## Slow Files Read & Writes

Turn on `spark.speculation = true` to allow more Tasks to write files. But add more deduplication steps at the end to avoid duplicated data created by this. 

---

## Slow JDBC Read

[[0_pages/01/2023-02-27-22-43-32-45400|Spark JDBC Parallel Read]]

---
## Failure due to Memories 

Excess use of [[0_pages/03/2023-02-26-17-20-20-43600|cache]] would lead to both memory and performance issues because of [[0_pages/01/2023-02-26-23-25-03-79100|full garbage collection]]. One way to avoid this is plan what/ when cache is needed; [[0_pages/01/2023-02-26-23-26-15-82900|uncache]] when no longer needed.

### Symtoms
- memory error blocking causing Spark to fail
- lots of time used for non-tasks related computation (i.e., garbage collections)




---
## ℹ️  Resources
- [[Apache Spark The Definitive Guide#^chapter-18]]
- [Spark - Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html#join-strategy-hints-for-sql-queries) 