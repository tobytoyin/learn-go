---
creation-date: 2023-01-06T22:30:17
type: walkthrough
aliases:
  - RDD Partitions Operations
  - partition mapper
  - partition transformation
tags:
  - technology/spark/pyspark
  - technology/spark
  - example
catalogs:
  - 2023-01-05-20-58
---

## Mapping Over RDD vs Partitions

While [[RDD Transformation|‚ÑπÔ∏è¬† Resources]] applies [[0_pages/02/2023-02-26-15-39-18-14900|Mapper Function]] on a single RDD; *Partitions RDD* applies operations on a single partition. This is because Spark [[0_pages/03/2023-02-26-15-40-01-93300#^35a0ea|parallelises computation on a partition basis]], thus functions can be applied on the entire partition as a single unit.

![[3_hidden/_images/Pasted image 20230107131622.png]]

Because RDD has the property to keep its partitions within the Object, a `MapPartitionsRDD` object universally has the typing of `Tuple[partition_key, Row]`

---
## When to Use Partition-wise Transformation?

*Partition-wise Operation* is useful when we need to apply different algorithms against different partitions. 

This means that ==data we executed against are partitioned by use logically columns== instead of randomly partitioned by Hash Index. 

For example, applying a different Machine Learning algorithms to different set of partitioned data; or as in this example, applying a currency conversion based on different country partitions. 

> [!example]- Scenario in this Note
> In the rest of this walkthrough, Partition Transformation is showcases by a scenario. This scenario, where we have a dataset that contains different countries‚Äô products using different currency. We want to apply to concurrency conversion algorithm to each partitioned data (partitioned by country).
> 
> ![[3_hidden/_images/Pasted image 20230107120951.png]]
> ---
> To setup the RDD partitioned properly, we first implement a custom [[0_pages/04/2023-02-26-17-20-55-53700|Partitioner]] to shuffle the dataframe into 3 separate parallelised partitions (one partition for each country):
> 
> ![[3_hidden/_images/Pasted image 20230107121352.png]]
^403a07

---
## `mapPartitions`

`MapPartitions` is the core function when we need to apply a function to partitions and pipe its output to the next stage. The input type for the function is a `partition` object `Tuple[key, RDD]`, we can examine it by: 

```python
# use MapPartition
def get_partition_records(part: Iterable):
    # convert iterator partition objects into concrete list
    return [list(part)]
    
map_partition_rdd = country_rdd.mapPartitions(get_partition_records)

for i, partition in enumerate(map_partition_rdd.collect()):
    print(f"Part {i} - {partition}")
```

![[3_hidden/_images/Pasted image 20230107120022.png]]

---
## `foreachPartition`

`foreachPartition` also apply a function to partitions but doesn't not output to the next stage. This is commonly used as a termination stage operations and use as a connector for each partition to some external storage. 

---
## `glom()`

RDD by defaults a flatten in a single iterator when `collect`. In some of the use cases, we might want to ==retain the partitioned form during the Action Phase==. 

Although this can be done using [[#`mapPartitions(partition)`|MapPartitions]], `glom` provides a more convenient way to achieve the above, as this method converts all the Executor's outputs into a list of objects (assuming that each Executor store 1 partition): 

```python
# This is the same as using `glom`
for i, partition in enumerate(country_rdd.glom().collect()):
    print(f"Part {i} - {partition}")
```
![[3_hidden/_images/Pasted image 20230107120330.png]]

---
## Apply Algorithm to Partitions 

With the understanding of the basics of Partition-wise operations, we can apply this into our scenario, where we have a dataset that contains different countries‚Äô products using different currency. We want to apply to concurrency conversion algorithm to each partitioned data (partitioned by country). See [[#^403a07|setup]].

```python
from pyspark.sql import Row

# the rate to convert based on the partition's key
conversion_rate = {
    'uk': 1., 
    'us': 0.83, 
    'eu': 0.88
}

# a helper function to add new property to Row()
def add_property(row: Row, **props):
    """Add new property and form a new Row"""
    return Row(**row.asDict(), **props)

def currency_conversion(part: Iterable):
    """Return a new RDD with a new currency property"""
    for part_id, row in part:
		# calculate a normalised price
        convert_rate = conversion_rate.get(part_id)
        convert_price = row.price * convert_rate
        
        # generate a new Row() object
        new_row = add_property(row, norm_price=convert_price)
        yield (part_id, new_row)
```

If we examine the result, this works as intended. Though this is a simple structure to apply algorithms to the entire partition, the same follows if we need to construct something more complex. 

![[3_hidden/_images/Pasted image 20230107130805.png]]


---
## ‚ÑπÔ∏è¬† Resources
- [[Apache Spark The Definitive Guide#^chapter-12]]
- [[üîñ learn-spark|Current Location]] ([zpln nb](https://github.com/tobytoyin/learn-spark/tree/main/pyspark))