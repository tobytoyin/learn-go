---
id: CE9CACED-ED17-4E25-9FE0-6D2DABCB6D57
creation-date: 2023-02-11T13:24:50
type: concept
aliases:
  - Messages Passing
  - pooling
  - connectivity
tags:
  - machine-learning
  - machine-learning/graph-model
  - machine-learning/neural-networks
catalogs:
  - 2023-02-10-22-54
---

# Messages Passing 

## Graph Connectivity

When taking *connectivity* into consideration when building the [[0_pages/02/2023-02-26-17-06-30-81300|GNN Layer]], the strategy being used is called *Message Passing*. 
This involves using a *Pooling Function* $\rho$ to add additional information from other connected neighbours' embeddings into an input embedding. 


![[3_hidden/_images/Pasted image 20230219181030.png|600]]^message-passings

1. For each Node/ Edge, gather its connected [[0_pages/05/2023-02-26-23-19-27-95200|embeddings]] (i.e., *messages*)
2. Aggregate the messages into a Pooling Function $\rho$ (e.g., summation)
3. Pass the pooled embedding forward/ into an Update Function $f$
	- usually expressed as $h' = f(h, pool(h_u))$ 

### Different Types of Neighbours

By considering "neighbours" of a graph's component, it can be categorised into 3: 
- [[#Messages Passing within Components|Nodes Neighbours]] - nodes-nodes connections
- [[#Edge Neighbours Messages Passing|Edge Neighbours]]:
	- nodes-edges connections
	- edge-nodes connections
- [[Global Neighbours Messages Passing|Global Neighbours]] - nodes/ edges that are connected $n$-traversals aways

---
## Nodes Neighbours Messages Passing

![[3_hidden/_images/Pasted image 20230219181057.png|600]]^node-passing

The most common Message Passing is based on the *Node-Nodes connections*. This aims to conduct a basic information pooling from connected neighbours Nodes. In this example, we have four Nodes with 4 Nodes embeddings $h_{\{1, 2, 3, 4\}}$: 

To take the pooling of Node 1 as the first iteration: 
1. $v_1$ connects to $v_{\{2, 3\}}$ but not $v_4$
2. pooling is done by summation: $h_1 + h_2 + h_3$
3. pooled embedding $\rho(h_1)$ is then passed further forward, e.g., Update Function

---
## Edge Neighbours Messages Passing

When learning for *edge representation* and add in connectivity, message passing is done by pooling neighbour nodes into an Edge. For example: 
- a Node is connected with different Edges
- an Edge is connecting 2 Nodes

![[3_hidden/_images/Pasted image 20230219180928.png|500]]^edge-mp


The Message Passing for Edges can either be: 
- *Node pools Edges* $\rho_V$ - add connected Edges information to a Node
- *Edge pools Nodes* $\rho_E$ - add connected Nodes information to an Edge

> [!NOTE]- Dimensions Differences
> The major challenge in here is that the dimension of Node embedding may not be the same as Edge embedding, e.g., Nodes and Edges have different number of properties. So that the pooling function cannot be just a simple summation. Therefore, message passing for edges-nodes usually involve learning a simple linear function $Wh_E + b$

![[3_hidden/_images/Pasted image 20230219181805.png|600]]^node-edge-passing

Let's consider 1 example - passing Edges messages to a Node 1 using a *Edge-then-Node Learning*: 
1. Node Embedding $h_1$ is transformed by a learnable function $g_V$ to have the same dimension as Edges
2. pooling of connected Edges by summation: $h_a + h_b + g_V(h_1)$
3. transformed the pooled Edge embedding back into Node's dimension by another learnable function $g_E$
4. feed the pooled Node Embedding further forward

> [!Tip]- Different Learnings Strategy
> "Edge-then-Node Learning" is type of learning strategy to pool information across different connectivity, there are many different combinations to do this, commonly:
> - *Edge-then-Node* - transform Node into Edge; pool Edges; transform back to Node
> - *Node-then-Edge* - transform Edges into Nodes; pool Nodes; transform back to Edges

---
## ℹ️  Resources
- [A Gentle Introduction to GNN](https://distill.pub/2021/gnn-intro/#comparing-aggregation-operations)