---
id: 9AC420FD-9F8F-4488-9A95-8186AAAF05DD
creation-date: 2023-02-10T22:42:04
type: concept
aliases:
  - Graph Neural Network
  - GNN
  - GNN Layer
tags:
  - machine-learning
  - machine-learning/graph-model
  - machine-learning/neural-networks
catalogs:
  - 2023-02-10-22-54
---

# GNN - Basic Building Blocks 

The basic building blocks of a *Graph Neural Network* (GNN) composes of two main steps: 
- GNN Layers - to learn embeddings (representation) of each Node, Edge, Context
- NN Layers - a classic neural network layer that is task/ prediction specific 

![[3_hidden/_images/Pasted image 20230214222600.png]]

> [!Tip]- MLP Reminder
> Given some input $X$, training it in an Update Function $f_X(X)$, which outputs an approximate value $X'$

---
## GNN Layer

![[3_hidden/_images/Pasted image 20230219182159.png]]

A *GNN Layer* is a hidden layer that learn an *Update Function* $f_G$ that can transform one embedding to another embedding (see [[0_pages/05/2023-02-26-23-19-27-95200#Embeddings Notations|Notations]]) independently.

In the simplest case, without considering "[[0_pages/02/2023-02-26-23-19-48-97800|connectivity]]" among the Graph, a GNN is like 3 independent NNs each handling Nodes, Edges, Contexts. 
Specifically, an embedding ($h$) is transformed by the Update Function into a new/ better embedding ($h'$). This approach is called "*graph-in-graph-out*": 

- $h'_v = f_V(h_v)$ - Node embedding in, new node embedding out 
- $h'_e = f_E(h_e)$ - Edge embedding in, new Edge embedding out 
- $h'_u = f_U(h_u)$ - Context embedding in, new Context embedding out 

Most of the time, these *Update Functions* are: 
- **shared and learnt for a single components**, e.g., one Node's Update Function for all Nodes
- **iteratively learnt**, e.g., single Node in, single Node out
- usually in a classical form, i.e., $f(Wx + b)$, where $x$ is some additional transforms

> [!NOTE]- Multi-GNN Layers
> Just like "deep" MLP, these GNN Layers work the same way which can *stacked* into multi-layered GNN as "deep GNN". 
> ![[3_hidden/_images/Pasted image 20230214230505.png|400]]

---
## GNN Activation Layer

> [!Tip]- Notation
> - $\phi_G$ - activation wrt to unspecified Graph's component
> - $\phi_V$; $\phi_E$, $\phi_U$ - activation function for Node, Edge, Context

A GNN *activation layer* usually is another [[neural networks architecture]], which ends with an differentiable *non-linear function* to generate the final output.  
This makes it suitable to substitute different neural networks models for different tasks, e.g., regression or classification. 


### Simple Direct Predictions 

![[3_hidden/_images/Pasted image 20230219182414.png|600]]

The simplest prediction is a component-to-component prediction. Through an **iterative process**, each embedding ($h_G$) is feed into an *activation function* ($\phi_G$) to generate an output. This can be: 

- node embedding to node prediction - $\hat{y_v} = \phi_V(h_v)$
- edge embedding to edge prediction - $\hat{y_e} = \phi_E(h_e)$
- context embedding to context prediction - $\hat{y_u} = \phi_U(h_u)$

> [!Example]- Node-to-Node Prediction
> For example: "Node to Node" prediction, where each Node embedding ($h_v$) is feed iteratively into an Node's activation function ($\phi_V$) for a prediction ($\hat{y_v}$):
> ![[3_hidden/_images/Pasted image 20230219182242.png|600]]

### Pooling Share Predictions 

![[3_hidden/_images/Pasted image 20230219182224.png|600]]

In a direct prediction, it assumes that the graph components exist to make the same prediction (e.g., nodes exist to predict nodes' outcome). 
It is often that information of some component is missing and **need to include other components to make predictions**. For example:
- include Edges to prediction Node's value
- include Nodes to prediction Context's value, etc.

This type of prediction requires an additional *pooling function* $\rho$ to pool another component's embeddings into prediction. For example:
- $\phi_E(\rho_{V\rightarrow E})$ - pooling node embedding $h_v$ to edge embedding $h_e$ to predict Edge's 
- $\phi_V(\rho_{E\rightarrow V})$ - pooling edge embedding $h_e$ to node embedding $h_v$ to predict Node's 
- etc...

> [!Note]- Pooling & Messsage Passing 
> When the learnt embeddings ($h_G$) have the same dimension, pooling can simply be done by *aggregation functions* (e.g., sum, average, max, etc.). When embeddings are in different dimensions, this is usually done as [[0_pages/02/2023-02-26-23-19-48-97800|Messages Passing]] with a simple learning function.


---
## ℹ️  Resources
- [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)