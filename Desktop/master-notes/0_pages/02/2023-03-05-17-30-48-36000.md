---
creation-date: 2023-01-01T17:09:34
type: walkthrough
aliases:
  - Spark on Docker
tags:
  - technology/spark
  - containers
---
## Why Containerising Spark?

[[0_pages/05/d-2023-10-13-21-50-14|Containerising]] [[0_pages/04/2023-02-26-17-21-15-09800|Spark]] application using Docker provides us some advantages in development: 
- Cloud implemented Spark are based on containers 
- Better understand when implementing in [[0_pages/04/2023-02-26-17-22-14-60800|cluster mode]]
- ensure consistency between development & deployment

This walkthrough shows how we can implement and run a simple Spark application: a basic Spark that aggregates CSV files and printing it out. 

> [!note]- No Output Writing
> We are not writing the output from Spark because containerised files are transient, i.e., delete after container's runtime is completed. Most containerised/ cloud-based Spark allow us to write files into a persistent cloud file system.

---
## 1. Creating the Project

First we create a project that like similar to how a Spark application would be deployed into production environment: 

![[3_hidden/_images/file-tree-20231230101171125.png]]

We separate two main dir: `app` storing our actual project codes, `data` storing our local test data uses as inputs later on. 

Inside `app`, each directory stores the following:
- `src/...` - dependency python modules
- `main.py` - the entry of our Spark script
- `Dockerfile` - build image script
- `requirements.txt` - `pip3` libraries if needed 

---
## 2. Create Scripts

In here, we: 
- create one basic `module.func` that simply print out some text to acknowledge that imports are working properly
- install `numpy` and see if we can build this and use it properly
-  a simple Spark code (`main.py`) that takes input file path as argument

```python
import sys
from src.module import func
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import numpy as np

if __name__ == "__main__":

    print(len(sys.argv))
    if len(sys.argv) != 2:
        print("Usage: spark-etl [input-folder]")
        sys.exit(0)

    # create a spark session
    spark = SparkSession.builder.appName("SparkETL").getOrCreate()

    # read csv data from a s3 path
    df = spark.read\
	    .option("inferSchema", "true")\
	    .option("header", "true").csv(sys.argv[1])

    # do some simple aggregation
    df = df.withColumn("value", F.col("value").cast("int"))
    df = df.groupby("id").sum("value")
    df.show(10)

    print("Testing External Package is working: ")
    print("numpy :", np.sum([1, 2, 3]))

    print("Testing Module is working: ")
    func()

    print("jobs completed")

```

---
## 3. Dockerfile 

In the `Dockerfile`, we use the latest image of `apache/spark-py`, which provides a `spark-submit` endpoint by default or an interactive terminal using interactive (`-it`) mode. 

```dockerfile
# base image from official pyspark
FROM apache/spark-py:latest

WORKDIR /

# use root to install python3 & pip3 
USER 0
RUN apt-get update && \ 
    apt install -y python3 python3-pip && \
    pip3 install --upgrade pip setuptools

# install python dependencies
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# copy all the project files into WORKDIR
WORKDIR /opt/work-dir
COPY . .
```

---
## 4. Build Image & Run Container

With all these setup, we can create an image of this application. At the root this directory, we build the image starting from the Project folder `app/`: 

```bash
docker build -t my-spark app
```

Then we can run our image by mounting our test data inside `/data/sample-data.csv` to the container volume: 

```bash
# the command looks like: 
docker run \
	-v <local-path>:<container-path> \
	<image-name> driver main.py <container-file>

# in our scenario: 
docker run \
	-v `pwd`/data:/opt/work-dir/data  \
	my-spark driver main.py data/sample-data.csv
```

There are several keywords we are interested in: 
- `driver` - indicates this will be a `spark submit <entry-point> <args>`  container
- `-v` - mounting the local file to the container so that the containerised Spark can access the data

---
## 5. Results & Cleanup

From the terminal, we can see that the Spark run properly: 

![[3_hidden/_images/Pasted image 20230101164305.png]]

We can perform cleanup on completed containers and image: 

```bash
docker container prune
docker image rm apache/spark-py
docker image rm my-spark
```

---
## ‚ÑπÔ∏è¬† Resources
- [Tutorial: Running PySpark inside Docker containers](https://spot.io/blog/tutorial-running-pyspark-inside-docker-containers/)
- [[üîñ learn-spark|Current Location]] (`/pyspark/container-example`)