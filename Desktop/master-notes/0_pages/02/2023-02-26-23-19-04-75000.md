---
id: B58E6EE0-599C-4CD0-A211-A1692718203F
creation-date: 2023-02-18T16:23:57
type: concept
aliases:
  - Integrated Gradient
  - IG
tags:
  - machine-learning/neural-networks
  - data-analytics
catalogs:
  - 2023-02-18-22-50
---

# Integrated Gradient 

## Overview
*Integrated Gradient* (IG) is an approach to explain feature importances & influences of an input data. As Neural Networks generally are a black box model (features-in; predictions-out), IG provides a NN-based *sensitivity analysis* to indicate how each features change would affect the prediction results. 

### Interpretation
- higher $IG_j$ indicates higher importance in feature $j$
- lower $IG_j$ indicates lower importance in feature $j$

---
## Gradient as Learnable Information

> [!Tip]- Gradient Descent Learning
> When training a NN, Gradient itself represents the additional information a NN can further learn from particular feature input $x_j$ and update a learning function $F$. The more "well-learnt" a model become, the gradient gets smaller towards 0. This is essentially what [[d-2023-08-06-12-21-10|Gradient Descent]] does. 

With a trained model $F_X$, when a new input $x = [x_1, x_2, ..., x_j]$ feeds into a model, we can obtain generally 2 scenarios: 
- if a feature input is same/ similar ($x_j$) to the data $F_X$ generalised: 
	- $err_{pred}$ would be small **with little gradient to learn** 
- if a feature input is absence/ different ($x'_j$) to the data $F_X$ generalised: 
	- if the feature is not important - small $err_{pred}$ and little gradient to learn
	- if the feature is important - large $err_{pred}$ and **large gradient to learn**

![[3_hidden/_images/Pasted image 20230218230154.png]]^gradient-importance

In short, if a feature is important and contains lots of information in determining the correct prediction outcome, a slight difference between the "presence" and "absence" of it would lead a large gradient. 

---
## Aggregation of Gradients 

![[3_hidden/_images/Pasted image 20230219121755.png|500]]^scale

The above $x'_j$ and $x_j$ is expressed in an absolute terms in either "presence" and "absence". The idea of IG is about finding this "importance" on a numerical scale: 
- closer to $x_j$ - more "presence" of feature
- closer to $x'_j$ (baseline) - more "absence" of feature
	- since feature absence is not always feasible, it's often done as a comparison to a *baseline* value
- the level that transits $x'_j \rightarrow x_j$ is *interpolation* of feature: 
	- those are the steps rolling from the baseline to $x_j$ 
	- $x_{\alpha} = x'+ \alpha(x- x')$ 

We can interpret Integrated Gradient of feature $j$ ($IG_j$) as
> [!tip] Integrated Gradient
> the aggregation of Importance (i.e., gradient) along the scale of absence-presence of feature ($x_j$) for a single input $x$

### Interpretation

The higher $IG_j$ is for $j$ feature, generally mean prediction is sensitive to the change in feature's value and thus more important; whereas lower $IG_j$ indicates prediction is less sensitive to feature and less important. 

---
## Mathematical Intuitions 

By obtaining the **predictions at different interpolation level** $F(x_{\alpha})$, we can obtain the gradient $\nabla_{\alpha}$ of that particular prediction w.r.t feature $j$. This is an integral (sum aggregation) of such gradients w.r.t  feature $j$: 

$$
IG_j(x, x') = \int_{x'_j}^{x_j}\nabla_{\alpha} \\\ \delta_{x'_j} 
$$
By converting to a (0-1) scale for integrals and rescale it back to the feature scale: 
$$
IG_j(x, x') = (x_j - x'_{j})\int_{\alpha=0}^{1}\nabla_{\alpha} \\\ \delta_{\alpha} 
$$
![[3_hidden/_images/Pasted image 20230219122801.png]]^ig
For computers to do computation, this continuous integrals is converted into a *Riemann Integrals* approximation as $m$-stepwise summation gradient blocks $\nabla_k$. 
$$
IG_j(x, x') = (x_j - x'_j)\sum_{k=1}^{m} \frac{\nabla_k}{m}
$$

---
## ℹ️  Resources
- [TensorFlow Integrated Gradient](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients#compute_gradients)