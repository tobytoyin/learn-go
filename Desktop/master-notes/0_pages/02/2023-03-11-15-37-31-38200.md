---
id: C0A5DF8A-63E6-451B-BFD8-A7B2AB259BF7
creation-date: 2023-01-28T17:10:10
type: walkthrough
aliases:
  - transform
  - composition
  - dectorator
tags:
  - technology/spark/pyspark
  - use-cases
  - technology/spark
  - example
title: PySpark - Transformation Functions
---

# PySpark - Transformation Functions 


When writing PySpark it is quite often we group many [[0_pages/03/2023-02-26-15-40-01-93300|transformations]] into separate functions. This approach is useful to make each transformation testable as well as more purposely-coupled.

However, these functions are often disconnected from our chain of Spark transformation's functions. 

For example: 
```python 
# we often write our own "transformation" code for Spark DF
def my_transform(df, new_col):
    return df.withColumn(new_col, F.lit(100))

def add_constant(df):
    return df.withColumn('constant', F.lit(1))

tmp = data.select('*').where(someExpr)  # normal spark chaining
tmp = add_constant(tmp)  # can't chain
tmp = my_transform(tmp, 'new_col')  # can't chain as well
```

--- 
## Solutions 

In order to make chaining of Spark functions possible, there are several ways: 
- [[#Monkey Patching]]
- [[#`transform` Method]]
- [[#Decorator Approach]]

---
## Monkey Patching

"*Monkey patching*" the PySpark DataFrame means that we add an inherit method into the `DataFrame` class during the runtime. 
Beware - the function first arg must be of Dataframe type. This is because a class methods always have `self` method at the first position. 

```python
from pyspark.sql.dataframe import DataFrame

# monkey patch existing functions into DataFrame
DataFrame.add_constant = add_constant
DataFrame.my_transform = my_transform

tmp = data.add_constant().my_transform('new_col')  # chain method
```


---
## `transform` Method

PySpark provides a way to do something similar to `pd.apply(func)` by using the syntax of: `DataFrame.transform(func)`. This accepts function that takes the first argument of DataFrame type and return a DataFrame. 

This can be done by using Lambda as a callback; or [[0_pages/02/2023-02-26-17-13-55-82300|partial]] as a callback:

```python
# a standard transformer function
def myFunction(df, new_value): 
	return df.withColumn('new_col', F.lit(new_value))

# apply through a lambda/ partial callback
callback = lambda df: myFunction(df, 'lambda_val')
partial_callback = partial(myFunction, new_col='partial_val')

# use `transform`
data.transform(callback).transform(partial_callback)
```

---
## Decorator Approach

Another way to do this is by applying Python [[0_pages/02/2023-10-10-23-59-05-01700|decorator]]. The benefits of this are: 
- write functions normally with less boilerplates and callbacks
- consumed by `tranform` like other Spark's functions, e.g., `F.sum(...)`
- has a UDF-like setup

What this decorator does is to wrap an ordinary transformation function using aÂ `@decorator`Â rather than a Lambda callback.  
To create this decorator, we need to understand howÂ `transform`Â call the function:

1. `transform(func)`Â calls the input function by:Â `func(df, *args, **kwargs)`
2. `func(df, ...)`Â returns a DataFrame
`
```python
from typing import Callable
from pyspark.sql.dataframe import DataFrame

def df_UDF(f) -> Callable: 
    # standard decorator: decorator(f)  
      
    def transform_callback(*args, **kwargs) -> Callable:
        # callback for `transform`
        
        def df_transform(df) -> DataFrame:
            # df transform where the func takes place
            return f(df, *args, **kwargs)
        
        # return the callback with 1st arg being reserved
        return lambda _: f(_, *args, **kwargs)
    
    return transform_callback
```

- The 1st layer - a standard decorator signature
- The 2nd layer - to return a function that can be called byÂ `transform`
- The 3rd layer - to pass an input DataFrame; transform it; returns it

To use it, we simply put decorate a transformation function: 

```python
@df_UDF
def my_constant(df, col_name, constant):
    return df.withColumn(col_name, F.lit(constant))
    
# use it through transform
data.transform(my_constant('UDF_val', 5))
```

---
## â„¹ï¸Â  Resources
- [[ğŸ”– learn-spark|Current Location]] ([.ipynb](https://github.com/tobytoyin/learn-spark/blob/main/pyspark/functions/transform.ipynb))