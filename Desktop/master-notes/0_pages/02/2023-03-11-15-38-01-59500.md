---
id: 407D4E40-0C46-415D-8C2C-9EB8CAD24B23
creation-date: 2023-01-08T17:41:41 
type: concept
alias: 
- Spark Monitoring
- Spark UI
tags: [ technology/spark, optimisation ]
---

## Key Aspects to Monitor

In a Spark application, it common needs to be monitored on the: 
- processes - the hardware usage of the [[0_pages/04/2023-02-26-17-21-15-09800|cluster]]  
- *query execution* - information related to the [[0_pages/02/2023-03-05-17-29-59-25300|application lifecycle]]

---
## Spark Jobs UI

Spark Job Page provides an overview of all the [[0_pages/02/2023-03-05-17-29-59-25300#Job|Jobs]] run within a cluster. When you executed most [[0_pages/04/2023-02-26-21-03-31-88200|Spark Actions]] like `show` & `collect`, it would incur one Job.

For example, in this one: 
![[3_hidden/_images/Pasted image 20230109205644.png]]
- *Description* - usually provides the codes that run 
- *Submitted* - the timestamp that Spark started to compute. In a notebook, this is not necessary the time when a cell is executed because Spark is lazily evaluated
- *Duration* - the time elapsed to finish
- *Stages* - how many Stages involved. ==Also suggests Job complexity==
- *Tasks* - how many Tasks in the whole Jobs. 
- *(Skipped)* - sometime Spark skips Stages/ Tasks because it is cached or computed before, so that it is just referencing the results in-memory

---
## Spark Stages UI

Spark Job Page provides an overview of all the [[0_pages/02/2023-03-05-17-29-59-25300#Stage|Stages]] run within a cluster. For example: 

![[3_hidden/_images/Pasted image 20230109210926.png]]
- *Pool* - the group ID of a compute resource, e.g., [[0_pages/04/2023-02-26-17-21-15-09800|a Node]], [[0_pages/04/2023-02-26-17-22-14-60800|a Pod]], etc..
- *Submitted* - indicates the timestamp that Spark starts one Stage
- *Shuffle Read/ Write* - Because a Stage concludes when data are repartitioned into another Executor, this indicates the size of repartitioning of RDDs when moving data among Executors

---
## Executors 

Executors Page provides the "hardwares" summary of each [[0_pages/04/2023-02-26-17-21-15-09800|executor]]. 


---
## SQL/ DataFrame UI

The SQL Page provides the [[0_pages/03/2023-02-26-15-40-01-93300|DAG]] (Query Plan) of the each Job. The DAG provides various information:
- internal on how Spark creates [[0_pages/02/2023-02-26-14-29-52-84600|RDD]] operations based on the *Structured API* 
- resources & summaries of each Task & Stage
- how [[0_pages/04/2023-02-26-17-20-55-53700|repartitioning]] are being done

> [!NOTE]- DAG Example
> ![[3_hidden/_images/Pasted image 20230109214733.png|500]]
> In this part of the DAG: 
> - Spark Scans for cached/ computed RDD for reuse
> - Strategy used in Aggregation and it's related resources monitoring
> - Repartitioning Strategy ("Exchange" in this example) and the number of partitions it resulted in

*Performance Tuning* usually involves in this page by looking at how certain computations lead to inefficient repartitioning; metrics.. 

---
## ℹ️  Resources
- [[Apache Spark The Definitive Guide#^chapter-18]]