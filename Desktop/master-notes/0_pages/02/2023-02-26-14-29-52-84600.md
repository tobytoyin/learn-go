---
id: 687D7061-E614-45AF-B419-7A14B50B3E72
creation-date: 2022-12-12T15:16:22
type: concept
aliases:
  - Spark RDD
  - RDD
  - partition
tags:
  - technology/spark
  - programming
  - distributed-computing
catalogs:
  - 2023-01-05-20-58
---

# Spark RDD 
   
##  ？What is RDD?

*Resilient Distributed Datasets (RDD)* is a Spark object that contains the collection of immutable data. In essence, RDD is equivalent to a [[0_pages/03/2023-02-26-17-12-01-86700|partition]] within [[0_pages/04/2023-02-26-17-21-15-09800|executor]]. 

RDD is an API that maintains how partitions should be parallelised and applies function. When operating on RDD, there are two major distinctions: 
- `MapPartitionsRDD` 
	- is the actual core RDD which contains a partition's collection of data
	- one unit of `MapPartitionsRDD` is the same as one partition
	- each data is typed as `Tuple[partitionKey, Row]`
- `Row`
	- `pyspark.sql.Row`
	- is the Spark equivalent of Records, i.e., single data within a partition
	- this is the closest unit we can construct MapReduce-like development

> [!tip]- Similarity to MapReduce
> In MapReduce, files are broken down into single Records for independent execution. 
> It is akin to an Record in [[0_pages/02/2023-02-26-15-39-18-14900|MapReduce]] as a single unit of input and both are the unit of parallelism (RDDs in Spark, Records in MapReduce). The major difference is that RDD is "resilient", i.e., in-memory.
>

---
## Properties of RDD 

If we look at the [[0_pages/02/2023-03-05-17-29-59-25300|how Spark runs an application]], a computation Stage of a single partition contains 4 main components: 
<sup>(Note: there are 5 components when Stages need to perform Joins)</sup>

![[3_hidden/_images/Pasted image 20230105210045.png]]

RDD is basically the low-level implementation of the overall Spark API. Thus RDD contains 5 main properties that enable the above workflow: 

- *List of Partitions* 
	- the partitions that RDD belongs to along the computation graph
	- See [[0_pages/02/2023-03-11-15-37-42-19800|Mapping Over RDD vs Partitions]]
- *Compute Function* 
	- a function applying to a single Record or on the entire partition
	- See [[#Spark Engine & Compute Function]] 
- *Partitioner* 
	- a mechanism to perform repartitioning (optional depends on the Stage)
	- see [[0_pages/04/2023-02-26-17-20-55-53700|RDD Partitioners]]
- *Output Locations* 
	- output locations if Spark needs to write the results (optional depends on the Stage)
- *RDDs Dependencies*
	- any dependencies among other Records
	- dependencies usually exist when joins required (see [[0_pages/01/2023-02-26-17-16-59-62600|MapReduce Joins]] for similar mechanism)
	- see [[0_pages/03/2023-02-26-17-19-01-95400|Key-Value RDD]]

---
## Spark Engine & Compute Function

RDD implements the [[0_pages/03/2023-02-26-15-40-01-93300|computation approach]] we use in high-level Spark, which are [[RDD Transformation|Transformations]] & [[0_pages/04/2023-02-26-21-03-31-88200|Actions]]. 
- *Transformation* - lazy evaluated and transform on a single data instance
- *Action* - concrete gathering and partition of all the data

Compute Function works on different level of RDD also expects different inputs: 
- Row level - operates on Row data `func(row: Row)`
- Partition level - operates on Partition of Rows `func(part: Iterable[Row])`

---
## ℹ️ Resources
- [[Apache Spark The Definitive Guide#^chapter-12]]