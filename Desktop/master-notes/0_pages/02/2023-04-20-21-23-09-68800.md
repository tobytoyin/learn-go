---
id: E3C866E9-5A37-4BDF-A6D4-69B4EFD3E0E8
creation-date: 2023-04-20T21:23:09
type: idea
aliases:
  - Spark Context & Spark Session Terminology
tags:
  - technology/spark
catalogs:
  - 2023-04-20-21-24
---
![[3_hidden/_images/Pasted image 20230420213309.png]]^spark-session-context-explain

To explain the entire Spark cluster and its terminology: 
- *Physical Computer (Cluster)* - is the actual hardware of a computing system. This can be a single machine/ distributed system of many machines
- *Spark Cluster* 
	- aka `SparkContext` is the **virtual cluster** that is running within the computing resources
	- It is a "virtually" distributed engine because it computes using a **distributed approach** but it doesn't rely on how the hardware is being setup
	- therefore there can exist machines in a physical cluster that are NOT included in a [[0_pages/04/2023-02-26-17-21-15-09800|Spark Cluster]]
- *Spark Session* 
	- is an "Application Session" when an [[0_pages/02/2023-03-05-17-29-59-25300|Spark Application]] is launched within a Spark Cluster
	- therefore a SparkSession contains all the information within a SparkContext + additional modification that is application specific
	- i.e., **SparkSession = SparkContext + Application**


---
## ℹ️  Resources
- 