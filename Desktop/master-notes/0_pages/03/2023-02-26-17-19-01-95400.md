---
id: C156466D-5D5C-46A7-88B3-086BB744EB5C
creation-date: 2023-01-07T19:12:28
type: walkthrough
aliases:
  - Key-Value RDD
tags:
  - technology/spark/pyspark
catalogs:
  - 2023-01-07-19-29
---

# Key-Value RDD 

A *Key-Value RDD* operation is similar to how [[0_pages/02/2023-02-26-15-39-18-14900#^89e6ef|MapReduce extracts key during Mapping Phase]]. Key-Value Operation is usually the prerequisite step of different operations: 
- [[0_pages/04/2023-02-26-17-20-55-53700|repartition]]
- [[0_pages/01/2023-02-26-17-16-59-62600|joins]]
- [[RDD Aggregation|group by aggregation]]

These operations commonly need to group data that are dependent of each, by extracting and transforming keys of RDDs data, this brings together all the dee

---
## General Rule for Key-Value Operations

Most of the time, when working with key-value RDDs. It takes two steps: 
1. extract key - `keyBy(keyCreationFunc)`
2. transformation/ actions on value - `<some-operation>ByKey` & `mapValues`

![[3_hidden/_images/Pasted image 20230107193104.png|500]]^key-value-rdd

---
## Example

```python 
# we have an rdd variable looks like below: 
# [Row(id=1, value='a'), Row(id=2, value='b'), ...]

# we want to extract `id` as the key of each data
# An inefficient way to do it would be: 
# which we would need to explicitly return both the key and value
rdd.keyBy(lambda row: row.id)\
    .map(lambda row: (row[0], row[1].id * 100))\
    .collect()
# > [(1, 100), (2, 200), (3, 300), (4, 400), (5, 500)]

# we could just use mapValues, which works as a map function by 
# but apply the function only to the value
rdd.keyBy(lambda row: row.id)\
    .mapValues(lambda data: data.id * 100)\
    .collect()
# > [(1, 100), (2, 200), (3, 300), (4, 400), (5, 500)]
```



---
## ‚ÑπÔ∏è¬† Resources
- [[Apache Spark The Definitive Guide#^chapter-13]]
- [[üîñ learn-spark|Current Location]] ([zpln nb](https://github.com/tobytoyin/learn-spark/blob/main/pyspark/key-values-rdd_2HNC5M5PA.zpln))