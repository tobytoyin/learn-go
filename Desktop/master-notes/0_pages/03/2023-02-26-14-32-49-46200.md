---
id: D8C0E653-2418-4C87-8153-BCFAB59F4238
creation-date: 2023-01-21T22:26:00
type: walkthrough
aliases:
  - Pandas UDF
  - Vector UDF
tags:
  - technology/spark/pyspark
  - python
---


Generic PySpark UDF is a [[0_pages/02/2023-02-26-14-29-52-84600|RDD]] function that applies to row-wise operations. This means every partition applies an UDF function to a single row for computation. 

*Pandas UDF* is a vectorised UDF that apply vectorised operations on every partition as a in-memory columnar computation. This allows Spark to parallelised by Pandas DataFrame per partition, rather than parallelised by Rows. 

### Benefits
- execute in a more efficient vector (`pd.Series`)/ matrix (`pd.DataFrame`) form
- function contents are written in mostly Pandas syntax, which is more friendly when composing columnar formulas
- `pandas_udf` requires a `returnType` to allow [[0_pages/01/2023-03-13-21-39-34-16500|PyArrow]] to convert Python functions into a statically typed equivalent in other language

---
## Summary

Pandas UDF works similarly to generic PySpark UDF:
- *[[#Series-to-Series]]* - as a way to create new columns through arrays compute
- *[[#Iterators-to-Iterators]]* - as a way to implement row-wise unpack iteration
- [[#Series-to-Scalar]] - as a way to aggregate arrays into a single value
- [[#Grouped Map|Group-to-DataFrame]] - as a way to aggregate group-by DataFrame into another DataFrame

---
## Series-to-Series

A *Series-to-Series* UDF is similar to how generic PySpark functions work. The main benefit in defining this type of UDF is have a more Pythonic way of writing formulas then using the build-in Spark `sql.functions`, which SQL functions are often not suited for writing dense formulas. 

The most basic forms look like this: 
![[3_hidden/_images/Pasted image 20230122220844.png]]
- `@pandas_udf` - is a UDF register decorator with the indicated return data type
- `s1, ...` - are the input column(s)

Often functions allow additional arguments, we can use a second decorator: 
![[3_hidden/_images/Pasted image 20230122220922.png]]

Both of these can be used as an usual Spark functions: 
![[3_hidden/_images/Pasted image 20230122221110.png]]

---
## Iterators-to-Iterators

Sometime we still want to use loops/ maps to execute functions on a row-by-row iterations along with the context of an array. This UDF is similar to Pandas: 
- `for ... iterrows(cols)`  - row-wise unpack for loops pattern
- `df[cols].apply(func)` - row-wise [[0_pages/02/d-2023-09-27-12-14-03|functional programming]] pattern

*Iterators-to-Iterators* UDF is usually suitable for: 
- iterative compute around some state (i.e., broadcasted/ initialised objects state)
- iterative updating some state 

![[3_hidden/_images/Pasted image 20230122221655.png]]

- input type -  `Iterator[pd.Series, ...]` as an array's row elements inputs
- within the function, using a `for-yield` pattern or a functional `map` approach
- output type - `Iterator[pd.Series, ...]` as row elements of a single/ multiple columns 

---
## Series-to-Scalar

A *Series-to-Scalar* UDF general aggregate an array into a single scalar number. This is commonly used in multiple situation: 
- columnar aggregations, similar to Spark `F.sum`, `F.mean`, etc
- group-by columnar aggregation, similar to Spark `agg(F.sum(...))` 

![[3_hidden/_images/Pasted image 20230122223251.png]]

---
## Grouped Map 

A *Grouped Map* UDF (or as *Group-to-DataFrame*) allows complicated group-by calculation to be written in a vectorised way. This is similar to [[#Series-to-Scalar]] operation but differently aims to: 
- take input as a `groupby` DataFrame
- for each Group produce (i.e., map) a new DataFrame

This is commonly used in: 
- applying an algorithm/ model on a group-by data
- matrix transformation, i.e., group-by Dataframe into a new DataFrame

![[3_hidden/_images/Pasted image 20230122223744.png]]
- input type is a `pd.DataFrame` 
- output type is a `pd.DataFrame`
- output requires a `schema` of the output DataFrame. If is the same shape as the original one, this would be the original schema; If the output is a new DataFrame shape, we can use `copy` and create a desired output schema

---
## ‚ÑπÔ∏è¬† Resources
- [Pandas UDF](https://spark.apache.org/docs/3.1.2/api/python/user_guide/arrow_pandas.html#pandas-function-apis)
- [[üîñ learn-spark|Current Location]] ([ipynb](https://github.com/tobytoyin/learn-spark/blob/main/pyspark/pandas_udf.ipynb))