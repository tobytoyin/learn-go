---
id: 43C3042E-E8E4-4AE4-819E-3CF54297E426
creation-date: 2023-01-08T16:47:24
type: concept
aliases:
  - Optimising Spark Partitions
tags:
  - optimisation
  - technology/spark
catalogs:
---

# Optimising Spark Partitions 

## Repartitioning

*Shuffling* in Spark is computational expensive operation because every data needs to travel through the network to physically allocate to another Executors. 

There are major concerns when dealing the number of partitions: 

---
## Too Many Partitions
- too many partitions
	- although more partitions can increase parallelism
	- too many partitions would lead to slower shuffling if network is the major bottleneck

---
## Too Frequent Repartitioning
- too high repartition frequency
	- if partitioning are too random and too many different group by / filter operations
	- executors would need to shuffle data across the network frequently

---
[[0_pages/03/2023-02-26-17-19-37-01900|skewed partition]]