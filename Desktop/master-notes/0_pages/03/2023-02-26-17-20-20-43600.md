---
id: 47338180-4508-47B4-9A26-E245660ED5FC
creation-date: 2022-12-18T23:00:05 
type: concept
alias: ['Temporary Data', "Intermediate Memory", "cache", "checkpoint"]
tags: [ technology/spark, technology/spark/pyspark, optimisation ]
---

# Optimising Spark Temporary Data 

## Temporary Data as Optimisation?

Because Spark stores all the [[0_pages/02/2023-03-05-17-29-59-25300#Tasks|computation Tasks results in-memory]], any of the data created within a [[0_pages/02/2023-03-05-17-29-59-25300#Stage|Stage]] are non-reusable by default. This makes Stage prone to faults and re-computation. 

We might want to cache intermediate results as optimisation for several reasons:
- *improve data reusable* 
	- because cached data are shared across all executors
	- this makes data read much faster
- *avoid data re-computation*
	- this avoids having other executors to recompute/ [[0_pages/03/2023-02-26-17-19-57-90800|repartition]] cached data
	- this avoids having to recompute the same dataset across multiple sessions 

> [!tip]- Things to Store Temporarily
> Some data is useful to compute right at the beginning and store it temporarily. 
> - some group by sum, mean, etc that are commonly used as denominator
> - some matrix that is used often, e.g., correlations
> - higher level partition aggregation to avoid frequent repartitioning along the Stage (e.g., Company Sales > Regional Sales > Teams Sales)

---
## Storage Method

*Temporary Storage* are only available through RDD and there are two ways to do it: 
- [[0_pages/01/2023-02-27-20-36-07-97200#Cache|Cache]] - in-memory temporarily storage
- [[0_pages/01/2023-02-27-20-36-07-97200#Checkpoint|Checkpoint]] - on-disk temporary storage

---
## Waterfall Caching

One optimisation approach to use caching is to cache dataset in a *waterfall approach*. This approach is to cache the parent dataframe early, so that subsequent subset of that dataframe can just read from cache without recomputing it. 


---
## ℹ️ Resources
- [[Apache Spark The Definitive Guide#^chapter-19]]