---
id: 95E2F5E9-6DBC-4D75-B9EB-FBADD825FDC8
creation-date: 2023-10-05T19:58:41
type: idea
aliases:
  - StreamHandler for LLama in FastAPI
tags:
  - python
---

While  [[0_pages/02/d-2023-10-09-22-20-31|Designing a LLM API]] [[0_pages/01/d-2023-10-05-14-11-54|in FastAPI]], the problems that basic model only either:
- stream outputs in terminal as stdout
- response all the text once the model generates all tokens, which takes time

We can create a custom callback to yield the tokens into an async callback. Then pass this into FastAPI streamResponse. This create something similar to OpenAI `stream=True` capability. 



---
## ‚ÑπÔ∏è¬† Resources
- [Custom callback handlers | ü¶úÔ∏èüîó Langchain](https://python.langchain.com/docs/modules/callbacks/custom_callbacks)
- [Custom Response - HTML, Stream, File, others - FastAPI](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)
- [python - llama\_index's query answer returns None for streaming=True - Stack Overflow](https://stackoverflow.com/questions/76584319/llama-indexs-query-answer-returns-none-for-streaming-true)