---
id: 6DF810AC-EE79-4395-AD50-063961D49DB3
creation-date: 2023-03-20T21:58:22
type: idea
aliases:
  - Spark Submit into Notebooks
tags:
  - technology/spark/pyspark
---

## Scenario

In one of project I worked, we are using [[0_pages/02/2023-02-26-14-00-02-41200|EMR's]] notebook to run Spark code. There are several challenges with that: 
- for some reason, we cannot use EMR run-notebook to run them
- it is not ideal to convert tested notebooks into untested py scripts, i.e., what were being sign-off should remain untouched
- we need to use `spark-submit` as the standard way to [[0_pages/02/2023-03-05-17-30-48-36000|Spark on Docker|submit Spark jobs into the container]]. This is because we are using [[0_pages/05/2023-02-26-15-35-38-89100|Airflow Operators]] to interact with a K8s cluster

---
## üèÜ Goals 

The goal of this approach is that: 
- avoid writing other scripts to pipeline multiple notebooks, i.e., keep everything in the [[0_pages/05/2023-03-11-15-39-13-34900#2. Scripting DAG|DAG script]]
- minimise the amount of DAG's tasks created just to simplify things
- simple interface/ configs for other developers to quickly add new DAGs

---
## üìñ Challenges along the Way

- even by using `spark-submit` into a notebook launcher script. I faced a situation where different IPython kernel cannot find the current Spark Context created by prior notebooks. 

For example: 

```python

# this creates the initial sc
os.system('ipython nb1.ipynb')  

# this fails to reference the 1st sc created 
# and by singleton, it cannot be initialised again
os.system('ipython nb2.ipynb')  

```

---
## Implementation

In this approach, I am basically: 
- use `spark-submit` to trick the Interpreter to launch `sc` through a Python script
- within the script context, launch another `ipython` instance to run notebooks
	- ensure `shell` is created so that all notebooks share the same global context and can reference the same `sc` and `spark` (see [[#üìñ Challenges along the Way|above]])
- the entry of the script looks like `submit-nb nb1.ipynb nb2.ipynb ...`
	- this makes it easy to create a list of nb `['nb1.ipynb', 'nb2.ipynb']` inside Airflow and programmatically launch
	- these become the entry script arguments
	  `spark-submit <other-configs> nb1.ipynb nb2.ipynb` 

```python
# submit-nb.py *argv

import os
import sys

from IPython import get_ipython
from IPython.terminal.interactiveshell import TerminalInteractiveShell

# shell ensure that all ipython gets into the same kernel
shell = TerminalInteractiveShell.instance()
ipy = get_ipython()


def run_notebook(nb_path: str) -> None:
    """Run a jupyter notebook in path"""

    try:
        result = ipy.run_line_magic('run', f'{nb_name}')
    except SystemExit:
	    # allowable exit to skip the notebook 
	    pass 


if __name__ == '__main__':
    # accept argument like "submit nb1.ipynb nb2.ipynb ..."
    argv = sys.argv[1::]

    _ = list(map(evalute_path, argv))  # ensure .ipynb
    _ = list(map(run_notebook, argv))  # run all
```

---
## ‚ÑπÔ∏è¬†Resources
- 