---
id: 1F3BB030-7595-4F7D-9356-8D71C7EC383B
creation-date: 2022-12-25T18:07:04 
type: concept
alias: 
- 'Spark Cluster Lifecycles'
- 'SparkSession'
- 'SparkContext'
tags: [ technology/spark, devops]
---

# Spark Cluster Lifecycles 

To deploy and start a [[0_pages/02/2023-03-05-17-30-33-26300|Spark application]], a Spark programme contains 4 lifecycles:  
- [[#Client Request & Deployment]]
- [[#Launching "Spark Cluster"]]
- [[#Execution]]
- [[#Shutting Down]]

> [!tip]- Cloud Implementation
> Cloud providers provide many managed service that include Spark. The overall lifecycle and deployment are conceptually similar.

---
## Client Request & Deployment

![[1_catalog/14/2022-12-25-18-26#^66aba9]]

Assuming that there's a running cluster (or launched before the deployment). A client software outside of a running Cluster Network would make a deployment using `spark-submit`. `spark-submit` usually contains several important arguments: 

| Arguments | Meaning |
| --- | --- | 
| scripts & scripts args | the compile script and other arguments for the script |
| config  | different Spark/ Hadoop configs | 
| master URL | the address of the cluster's Driver Node for connection | 

These arguments allow Spark to connect to a Cluster, then allocate one Node to run the [[0_pages/04/2023-02-26-17-21-15-09800#^8776b7|Driver Process]]. This Node would contains all the scripts and dependencies.

---
## Launching "Spark Cluster"

![[1_catalog/14/2022-12-25-18-26#^72090e]]

Once the Driver Process has been setup, it would start the run the Spark script: 
1. Spark's `SparkSession`/ `SparkContext` would request [[0_pages/04/2023-02-26-17-21-15-09800#^76d11d|Cluster Manager]] to initialise a [[0_pages/04/2023-02-26-17-21-15-09800#Cluster vs Spark Cluster|Spark Cluster]]
2. Cluster Manager would allocate Worker Nodes to host [[0_pages/04/2023-02-26-17-21-15-09800#^03d370|Executor Process]]
3. Cluster Manager would return meta information (e.g., Executor's Worker Nodes addresses) to the Driver Process. This allows the Driver to communicate with Executors directly later
**
> [!NOTE]- What is `SparkContext`
> `SparkContext` is the entry point of a Spark application, i.e., the *orchestrator of the Spark cluster*. It is responsible to maintain cluster-wise connections, e.g., connections with the client (external) & connections between Driver and Executors (internal). See [[0_pages/02/2023-04-20-21-23-09-68800|here for illustration]]

---
## Execution

![[1_catalog/14/2022-12-25-18-26#^a7659f]]

Once the Spark Cluster has been setup, ==Driver Process would contains the address of each Executor== within the cluster. This allows Spark Driver to generate computation steps & tasks based on the script; allocate tasks to different Executors; monitor Executors' states. Once Executors finish their task, it would send the results back to the Driver. 

---
## Shutting Down

![[1_catalog/14/2022-12-25-18-26#^668add]]

Once the job completes: 
1. Spark Driver would exit the application and return the exit states back to Cluster Manager
2. Cluster Manager would shut down the Executor Processes assigned in the cluster's Worker Nodes

> [!note] 
> The Cluster don't necessarily need to shut down because the Spark is just a running process inside an existing cluster.


---
## ℹ️  Resources
- [spark-submit documentation](https://spark.apache.org/docs/latest/submitting-applications.html)
- [[Apache Spark The Definitive Guide#^chapter-16]]