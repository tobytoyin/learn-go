---
id: C8F71FD9-8EEB-485F-A1B0-D9DB4EF8928B
creation-date: 2023-02-27T22:43:29
type: idea
aliases:
  - Spark JDBC Parallel Read
  - jdbc
tags:
  - technology/spark/pyspark
  - optimisation
---

When doing JDBC read, the default behaviour is to have one executor connecting to one JDBC client (i.e., single partition). This is because external SQL tables' internal partitions are unbeknownst to Spark. 

To ensure JDBC read can archieve parallelism, it needs to have `partitionColumn`, `upper/lowerBound` to perform [[0_pages/03/2023-02-26-17-13-11-48000|partitions scanning]] while making parallel connections. 



---
## Parallelism without suitable `partitionColumns`

We can create a numerical column during a JDBC request by these approaches: 
- manually hashing string key-value to enforce Hash Key Partitioning
	- SQL here >>>
- use/ create row id 
	- SQL here >>>
- use random numerical values to achieve even partitioning
	- SQL here >>>

---

> [!Question] Why only Numerical Partitioning is allowed?
> To repartition string key-values from JDBC, parallelism would be [[0_pages/03/2023-02-26-17-13-11-48000#Easy to Perform|inefficient to archive without knowing the underlying structure]] of the JDBC tables. This is because Spark's clients would need at least two queries in order to perform [[0_pages/05/2023-02-26-15-41-40-19700|Distributed Requests]]: 
> 1. query to get all distinct key values
> 2. query to pull the JDBC table
> 
> But for numerical values, these values can be obtained by a single query: 
> 1. sort the JDBC table by numerical values
> 2. query to pull the JDBC table




---
## ℹ️  Resources
- 