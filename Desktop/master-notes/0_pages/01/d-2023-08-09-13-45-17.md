---
id: AB0C9E36-C26F-4AC3-8DC6-3E7EE2461431
creation-date: 2023-08-09T13:45:17
type: walkthrough
aliases:
  - DBT Cloud with Snowflake
  - DBT DAG
tags:
  - setup
  - automation
  - snowflake
---

DBT is a [[0_pages/05/2023-03-05-17-28-36-24300|Workflow Automation]] tool that enables process automation within [[0_pages/02/2023-02-26-17-10-39-78700|Snowflake]], there are several key capabilities: 
- enable simple DAG construction among SQL scripts using *script reference*
- *parameterised source tables* reference across SQL scripts

An overview of the DBT and Snowflake workflow & responsibility is as followed: 

![[1_catalog/14/c-2023-08-09-20-08#^dbt-sf]]

- [[#Setup Repository|Code Repository]] - to maintain DevOps tasks & storage for models (SQL) logics 
- [[#Setup Snowflake Databases|Snowflake Warehouse]] - as the platform's resources to compute and store results
- [[#Setup DBT Connections|DBT Server]] - an local/ cloud server to act as a scheduler to execute codes through a common interface to different databases connector. Put it simply, it **coordinates to run repo's codes in Snowflakes**

---
## Setup Snowflake Databases

Once the connection has been set, Snowflake would populate a new database that stores results executed within DBT: 
- Database - DBT project database, or point to an existing database
- Schema - `DBT_<name>` (convention)

This new (existing) database would be used to store models, results that are generated by DBT. For example, by using `dbt run` all the materialised objects would be stored under this `database.schema...` tree.

---
## Setup Repository

Code Repository is used to stored codes (parameters, infra configs, SQL scripts) that are used and executed on Snowflake. This can be importing from an existing Github repository, or a DBT maintained repository. 

---
## Setup DBT Connections

![[3_hidden/_images/Pasted image 20230809140547.png|500]]

There are couple of fields we need to setup: 
- Repository - sync the code repo with DBT cloud's repo
- Connection - connection to a (cloud) data warehouse/ databases. In this [[0_pages/02/2023-02-26-17-10-39-78700|Snowflake]] setup, we would need: 
	- the account URL - e.g., `tj83216.eu-west-2.aws` from [Account Identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html)
	- specific Role (if any) to use when interacting with Snowflake objects
	- specific Snowflake Database to store DBT materialised result tables
	- specific Warehouse to use for computation

---
## Turning SQL Results into Tables

Whenever we create a `.sql` file inside `/models`, this will be run by DBT as a model's tables. In order to turn the final query of each SQL file into a [[0_pages/02/2023-02-26-22-05-18-97000#^47272f|Materialised Table]] (Table updated per event). We need to modify the configuration, e.g., 

```yml
# dbt_project.yml
...

models:
  <project_name>: 
    +materialized: table
    <models/subdir>:
      +materialized: view
```

Each **Materialised Table created in Snowflake are named after the SQL script name**, and the materialised Tables are stored inside DBT's named project database. 

---
## Building SQL DAG 

When the models scale, there will be more and more SQL scripts that rely on sequential execution in a form of DAG. In DBT, sequential orders among the SQL scripts can be implicitly defined and DBT will automatically construct the DAG. E.g., 

![[3_hidden/_images/Pasted image 20230809174614.png|500]]
We have three SQL scripts, where `customer.sql` is dependent on `stg_customers.sql` and `stg_orders.sql`. We can use "*script reference*" in successor scripts to construct such DAG: 

```sql
-- models/customer.sql
with customers as (
    select * from {{ ref('stg_customers') }} -- ref .sql
),

orders as (
    select * from {{ ref('stg_orders') }}
),

-- do customer.sql SQL logics
```

---
## Maintain Snowflake Source Table

Although we can use `Database.Schema.Table` within the SQL to select our source Table for the queries, this is not ideal when we might need to change lots of script when the source changes. 

*Source* - `source(name, table)` - can be defined in `models/sources.yml` as parameterising these Tables reference. Therefore if the sources has changed, only the config file needs to be changed without changing the SQL scripts.  ([see here](https://docs.getdbt.com/reference/resource-configs/snowflake-configs))

---
## ℹ️  Resources
- [Quickstart for dbt Cloud and Snowflake | dbt Developer Hub](https://docs.getdbt.com/quickstarts/snowflake?step=4)
- [A Git Workflow for Data Teams](https://www.getdbt.com/analytics-engineering/transformation/git-workflow/)