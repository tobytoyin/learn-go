---
id: F914FD62-D35B-408F-A61C-C17C59DEA9C5
creation-date: 2023-10-11T22:39:27
type: summary
aliases:
  - Understand K8s Cluster Setup
tags:
  - k8s
  - setup
---


A [[0_pages/05/2023-02-26-23-21-57-15700#Cluster Architecture|K8s Cluster architecture]] requires multiple machines (Nodes), how does it work and setup, such that a K8s cluster can connect with different Nodes?

To understand the setup and start of a K8s Cluster, below are the steps involves to: 
- setup multiple Nodes to prepare Nodes for a K8s cluster
- setup Nodes' networks to maintain a K8s cluster

---
## Adding "Known Hosts" to Nodes

![[1_catalog/14/c-2023-10-11-22-41#^step-1]]

When working with Linux Nodes, we need to update a set *Known-Hosts* in each Nodes. This allows Nodes to **recognise the traffics from those hosts' IP** are valid. This also add a benefit to know what the current host via terminal (`user@hostname`): 
1. set up a designated hostname in each Node
	- `sudo hostnamectl set-hostname <hostname>`, e.g., `control`, `worker-1`, etc.
2. update the `/etc/hosts` file to add Known Hosts in each Node by adding: 
	- `<master-private-ip> <master-hostname>`
	- `<worker-private-ip> <worker-hostname>`
3. restart the terminal to take effect 

---
## Installing K8s Dependencies

To start a K8s cluster, all the Nodes need to have the following installed and configured: 
- a container engine (e.g., ContainerD, Docker engine, etc.)
- `kubelet` - Node's agent to interact with the Cluster
- `kubeadm` - K8s Cluster setup 
- `kubectl` - K8s cluster control plane

Many software managed and helped to configure all the settings underneath the Linux Kernel. 

---
## Setting a Cluster

![[1_catalog/14/c-2023-10-11-22-41#^start-cluster]]

The first step is to start a Cluster (without any Workers attached to it), this involves a single Node (as control-plane) in the Control Panel: 

```shell
sudo kubeadm init \
	--pod-network-cidr <pod-virtual-ip-range> \
	--kubernetes-version <version>
```

- starts a cluster without any workers
- `--pod-network-cidr` defines the virtual IP range for each Pod within the cluster

We can check the cluster only has one Control Node inside 

```shell
kubectl get nodes
```

---
## Attaching Worker Nodes to Cluster

![[1_catalog/14/c-2023-10-11-22-41#^k8s-cluster]]

1. install a network module for the Cluster to connect to workers, e.g., `calico`
2. create a command for Workers to register to the Control Panel and add into the Cluster - `kubeadm token create --print-join-command`
3. copy the above command and run at each Worker - `sudo kubeadm join ...`. This indicates the Worker Node makes a handshake with the Cluster and attach into it

If we run `kubectl get nodes` again, it will show that the cluster now has some additional Workers within the cluster. 

---
## ℹ️  Resources
- 