---
creation-date: 2022-12-29T18:20:53
type: concept
aliases:
  - Apache Airflow
  - Airflow DAG
  - workflow
tags:
  - airflow
  - devops
  - automation
catalogs:
  - 2023-01-02-14-21
---

*Apache Airflow* is a programmatic way to script and [[0_pages/05/2023-03-05-17-28-36-24300#WMS|manage pipeline and workflow]] using Python. This software allows developers to: 
- organise tasks & pipelines
- create CRON schedules
- manage tasks/ pipeline dependencies

All of this workflow management is based on the concept of a Workflow Graph, i.e., *Directed Acyclic Graphs (DAG)*.

---
## Architecture

![[3_hidden/_images/Pasted image 20230505113445.png]]

Conceptually, Apache Airflow works like this: 

- a *Database* that stores different workflow components
	- (mostly) everything within Airflow is a **query against the Database**
	- e.g., DAGs, schedules, metadata & variables, Tasks' outputs

- a running *Server* that continuously interacting with the Database and Scheduler to check if workflows need to be executed
- a *virtual environment* to execute the workflow through *Operators*
	- each Operator would launch a [[0_pages/03/2023-02-26-17-12-42-45600|distributed]] Executor
	- e.g., new terminal; container environment that can run Python

---
## Airflow Components

There more some commonly used *Airflow Components* (as Python Objects) in designing a workflow DAG: 

### DAG Workflow

is the main pipeline that defines how the steps should be executed and Tasks dependencies. It also contains many metadata, like *scheduled time* and *namespaces*; event-based functionality like *retries scenario*, *notifications*, *events callbacks* etc.


### Task

![[3_hidden/_images/Pasted image 20230423183536.png]]

*Task* is a group of operations execute within an isolated environment/ machine, this usually comes as an *Operator*. An Operator generally encapsulates a specific purpose/ tool, e.g., `BashOperator` executes bash; `PythonOperator` executes Python.

 Each Task is entirely isolated from others and don't share any internal states. To pass on Tasks' output/ states to another Task(s), it needs to use [[2023-04-23-19-27-27-18800|XCOM]] or [[0_pages/01/2023-04-17-21-58-42-23200|Variables]] to publish states to others. 

---
## Scripting Approaches

Airflow allows us to define workflow in both OOP and Functional Approach. 
- [[0_pages/05/2023-03-11-15-39-13-34900|Object-oriented]] - classic Airflow which uses classes and context manager to define workflow steps
- [[0_pages/04/2023-04-15-22-30-55-70800|Functional]] - Taskflow API uses decorators to define workflow steps using procedural functions chaining



---
## ℹ️  Resources
- [Apache Airflow Fundamental Concept](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html)