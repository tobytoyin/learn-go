---
id: 909593E9-1E59-42B0-AE2B-3F67D8CD3EA7
creation-date: 2023-07-30T18:58:38 
type: concept
alias: 
- Reinforcement Learning
tags: 
- machine-learning
- basics 
---

## What is Reinforcement Learning

In contrast to Supervised Learning, *Reinforcement Learning* (RL) aims to apply machine learning when the results & objectives are delayed and don't have an immediate respond: 
- results are delayed after period of time
- results are delayed after series of steps

A traditional supervised learning cannot be effectively trained because those models expect to learn from a one-time-prediction; whereas RL models focus on learning to **achieve expected result across sparse actions within a context**.

---
## RL Environment

![[1_catalog/14/c-2023-07-30-20-54#^rl-environment]]

An RL Setting is a simulated environment for AI to execute Actions freely on a sandbox environment. This involves certain concepts: 
- *Agent* - a system that has the capability to:
	- interpret the output from a Policy's Model, then: 
	- execute & interact with an Environment (*Actions*)
- *Policy* - a RL model that processes inputs and generate decision. It generally contains two layers: 
	- Input Layer - which consumes Environment's *Observations* & (intermediate/ final) *Feedbacks* from the Environment if there are any
	- Output Layer - which is the model's output
- (Simulated) *Environment* - a bounded context which receives Actions from the Agent + interactions & mechanism of its own rules (usually hidden)
	- *Environment Objects* - objects that can be directly interacted by the Agent
	- Environment also generates *Observations* to the Agent, such as data captures from the objects; the contextual data of the environment that is uncontrollable by the Agent, etc..
	- It might generate intermediate Feedback (e.g., scoreboard) and when the Environment is terminated, it would generate a final Feedback

### Action-Feedback Loop

This cycle between the Agent and the Environment forms an *Action-Feedback Loop* which runs continuously until the Agent or the Environment stops. Each time an Agent conduct Actions during the loop, the Environment would respond with: 
- post-Action Observations - data of the Environment & its objects after an Action
- *Positive/ Negative Feedbacks* - Feedback when the entire cycle is completed; or if there are intermediate Feedback during the cycle after an Action

---
## RL Training Lifecycle

![[1_catalog/14/c-2023-07-30-20-54#^rl-training-cycle]]

The basics of training a RL models generally involves the following stages: 
1. Allow the Agent to use an initial Policy to conduct Actions steps. For each *Action Elapsed*, Observations and Feedbacks are passed to the Policy to maintain the Action-Feedback Loop + Feedbacks (with any intermediate) are collected to a [[#Policies Search|Policy Search Layer]] for model training
2. Once the Action-Feedback Loop is completed, this completes an *Episode* of RL Training. The *Policy Search Algorithm* would tune the Policy aiming to achieve better result (i.e., higher Positive Feedback)
3. Training is completed when all Training Episodes cycle end


---
## Policies Search

To train a RL model, the parameter tuning process is called a *Policies Search*. This involves a *Policy Search Algorithm* which tunes the Policy's parameters using Feedbacks collected during the Training Lifecycle to optimise for highest Positive Feedbacks.

Generally, a Policy Searching Layer in RL only takes the following inputs: 
- Feedbacks from each Action Elapsed
- Policy's current parameters
- (if any) correct Action for each Action Elapsed

*Feedback Optimisation* is commonly done by finding and optimising:
- Total Feedbacks in one Training Episode
- (if ant) Loss based on Action-Feedback in one Training Episode



---
## ‚ÑπÔ∏è¬† Resources
- [[üìï Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow#^chapter-18]]