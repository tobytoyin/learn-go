---
creation-date: 2023-01-02T16:27:33
type: walkthrough
aliases:
  - Airflow OOP Example
tags:
  - airflow
  - automation
catalogs:
  - 2023-01-02-14-21
---

Here is an example on how to run Apache Airflow locally as a quick start. There are some modification on the [original](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html#operators) tutorial in organising the codes better. 

---
## 1. Starting Airflow Database

```bash
# in the context of a python virtualenv
airflow standalone
```

---
## 2. Scripting DAG

To script the [[0_pages/05/2023-02-26-15-35-38-89100#Airflow Components|DAG]], we can create Python script at some directory (we will move them to appropriate location later). Our workflow is very simple: using Bash to print date then sleep for 5 seconds. 

![[3_hidden/_images/example-workflow-20230102163301.png]]

### DAG Context Object
First we create our DAG context as an Object, at the same time add one documentation to this DAG using `doc_md`:

```python 
# Construct the pipeline metadata & schedules
pipeline = DAG(
    "tutorial",
    description="A example workflow",
    schedule=timedelta(days=1),  # CRON every 1 day
    start_date=datetime(2023, 1, 2),
    catchup=False,
    tags=["example"],
    default_args={
        "depend_on_past": False,
        # notification settings
        "email": ["someone@example.com"],
        "email_on_failure": False,
        "email_on_retrty": False,
        # pipeline retries when failed
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
)

# Documentation of the Pipeline
pipeline.doc_md = """
    ## Pipeline Documentation
    This is an example pipeline implemented in apache-airflow:
    1. print out current date
    2. wait for 5 seconds
"""
```


### Task Objects 

Then we create our different [[0_pages/05/2023-02-26-15-35-38-89100#Airflow Components|Tasks]] using `BashOperator`, which allow us to execute bash commands based when scripting in Python: 

```python
task_1 = BashOperator(
    bash_command="date",
    task_id="print_date",
    dag=pipeline,
)
task_1.doc_md = dedent(
    """
    ## Task 1 Documentation
    Task 1 for this pipeline is to execute `date` in bash.
    """
)

# 2nd task - sleep in terminal for 5 seconds
# As in Bash: `$ sleep 5`
task_2 = BashOperator(
    bash_command="sleep 5",
    depends_on_past=False,
    task_id="sleep",
    retries=3,
    dag=pipeline,
)
```

One thing doing differently than in the official tutorial is that, we create these Tasks as a global scope objects instead of within the DAG scope. This keeps the DAG context's code clear. To indicate that these Tasks belong to a specific DAG, we need to put one kwargs `dag={defined_dag_class}`.

### Structure DAG

With both the DAG and Tasks components defined, we write/ structure our DAG: 

```python
# Construct the pipeline as DAG file
with pipeline as dag:
    # setup dependent tasks: do task_1 --> do task_2
    task_1 >> task_2
```

This indicates that our workflow starts from `task_1` then proceed to `task_2`.

---
## 3. Adding DAG to Database

Now that the script defines our workflow, we need to deploy it into a database. By default, DAG workflow are stored in `$AIRFLOW_HOME/dags` (see [[0_pages/01/2023-03-11-15-35-19-04000|Setup DAGs Directory]]).

```bash
# in-case the dir is not there
mkdir $AIRFLOW_HOME/dags 

# deploy script to airflow DB
cp <this-script> $AIRFLOW_HOME/dags

# start the database
airflow db init

# test the Tasks (we need to indicate a virtual starting datetime)
airflow tasks test <dag-name> <step-name> <start-datetime>
airflow tasks test tutorial print_date 2023-01-02


# test the whole Workflow
airflow dags test <dag-name> 
airflow dags test tutorial <start-datetime>
```

The output would show something like: 
![[3_hidden/_images/Pasted image 20230102165214.png]]

---
## 4. Cleanup 

Now that our DAG example is completed, we can remove the DAG from the database:
```bash
airflow dags delete <dag-name>
```


---
## ‚ÑπÔ∏è¬† Resources
- [[üîñ learn-spark|Current Location]] (`pyspark/airflow/example.py`)
- [Apache Airflow Fundamental Concept](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html)
