---
creation-date: 2022-12-31T17:29:50
type: walkthrough
aliases:
  - Spark Cluster Mode
  - Spark on Kubernetes
tags:
  - technology/spark
  - k8s
  - distributed-computing
  - cli
catalogs:
  - c-2023-01-04-14-08
---

# Spark on Kubernetes 

Running Spark [[0_pages/04/2023-02-26-17-21-15-09800|cluster mode]] on [[0_pages/05/2023-02-26-23-21-57-15700|Kubernetes]] has some advantages: 
- K8s has capability to auto-restore Pods, making it easier to retry failed Tasks
- Both Spark & K8s clusters utilise on *virtual Nodes* (i.e., multiple processes within Nodes), making Spark in [[0_pages/02/2023-03-05-17-30-48-36000|containers]] as a distributed system simply to deploy
- K8s configuration & scalability is more robust than simple containers
- Parallelisation is dependent on number of Pods instead of number of Workers. This **allows fewer computation resources** to be used whilst keeping higher parallelisation without under-utilising resources

---
## Architecture

![[3_hidden/_images/Pasted image 20231005231622.png]]

There are lots of architectural similarity between K8s cluster with a Spark cluster: 
- ***Pods & Executors***
	- Both are these are virtual nodes that hosted within an physical Node
	- ⭐️ Transiting from Spark to K8s means that each Executor & Driver are now deployed as [[0_pages/05/2023-02-26-23-21-57-15700|Pod]] from a Spark Image
- ***Control Panel & Cluster Manager***
	- K8s uses Control Panel to maintain and deploy Pod in each Node
	- this is similar to how Spark needs an external service that serves as a Cluster Manager
- Client & `spark-submit`
	- `spark-submit` can directly request as a cluster's API call to the Control Panel

---
## Walkthrough to `spark-submit` to K8s Cluster

Below is an example on how to run `spark-submit` in a running K8s cluster, we first would need to have a Kubernetes installed and cluster running first: 
- MacOS -  [[0_pages/04/2023-05-04-23-46-10-46900|Setting up K8s on MacOS]]

---
## 1. Start a K8s Cluster

We will be using `podman` as a container engine for our K8s and `minikube` as a K8s cluster engine: 

```bash
# setup the a basic virtue machine with suggested settings 
podman machine init --cpus 4 --memory 4096 --disk-size 20
podman start

# start a k8s cluster
minikube start \
	--driver=podman \
	--container-runtime=containerd \
	--kubernetes-version=v1.21.1
```

---
## 2. Setup ServiceAccount and Role Binding

Because Executors are deployed as Pods across the cluster, we need to have some way for all these Executor Pods to form a [[0_pages/02/2023-02-26-23-24-13-54500|Service]]. ==With a Service, Driver and Executors would be able to communicate Tasks within a cluster network==. 

> [!tips]- Knowing about Cluster Namespaces
> The suggested approach by Spark is to create a separate `serviceaccount` and bind the policy of the `default` cluster with the `serviceaccount`. 
> 
> Because Services in cluster is based on `labels` and `namespace`. This allows Spark to utilise the entire Service that already exist in the cluster.   
> For example: using `kubectl get services -A` , we get: 
> 
> ![[3_hidden/_images/Pasted image 20230104145251.png]]
> This indicates that `kubernetes` uses the `namespace=default`. By binding the new `serviceaccount` namespace with `default`, this essentially allowing the new ServiceAccount to utilise the whole cluster's resources.

```bash
# create a Service Account with `spark` namespace
kubectl create serviceaccount spark

# create a new role `spark-role`
# that bind the `spark` service account 
# with the `default` role policies
kubectl create clusterrolebinding spark-role \
	--clusterrole=edit \
	--serviceaccount=default:spark \
	--namespace=default
```

---
## 3. Run Spark Job in Cluster

Now we can run `spark-submit` through the K8s cluster. 

```bash
spark-submit\
	--deploy-mode cluster \
	--master k8s://<cluster-ip:cluster-port> \
	--name example-spark \
	--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
	--conf spark.kubernetes.namespace=default \
	--conf spark.kubernetes.context=minikube \
	--conf spark.executor.instances=5 \
	--conf spark.kubernetes.container.image=apache/spark-py:latest \
	local:///opt/spark/examples/src/main/python/pi.py
```

There are several key points in here: 
- `--deploy-mode cluster` - using a cluster mode to run Spark
- `--master` - the K8s entry point, we can use `kubectl cluster-info` to find this
- `...serviceAccountName=spark` - indicate a specific role (`serviceaccount`) to create Pods in the cluster
- `...namespace` - use the resource from a particular `namespace`
- `...container.image` - we use the default [PySpark Image](https://hub.docker.com/r/apache/spark-py) for our Pods, if we need to include more stuff, e.g., custom scripts and package, we can [[0_pages/02/2023-03-05-17-30-48-36000#3. Dockerfile|build our Dockerfile]]

---
## 4. Checkout the Logs & Results

Once the `spark-submit` finished computing, it will terminate the cluster with a `Completed` status: 
![[3_hidden/_images/Pasted image 20230104150650.png]]

Using the `submission ID`, we can check the logs and the print out results using: `kubectl logs  spark-pi-9e07b5857d0dc67e-driver` and get this: 

![[3_hidden/_images/Pasted image 20230104150748.png]]

---
## ℹ️  Resources
- [Running Spark on Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)