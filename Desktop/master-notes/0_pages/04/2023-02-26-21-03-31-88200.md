---
id: 8E871D02-B945-45B0-8229-BEA94408FFC9
creation-date: 2023-01-06T11:37:26
type: walkthrough
aliases:
  - RDD Actions
  - reduce
tags:
  - technology/spark/pyspark
catalogs:
  - 2023-01-05-20-58
---

# RDD Actions 

RDD [[0_pages/03/2023-02-26-15-40-01-93300|Actions]] are similar to the MapReduce's [[0_pages/02/2023-02-26-15-39-18-14900|Reduce Stage]], which it gather all the data to conduct *iterative computation*. These "Reducer side" methods need all the RDDs to be available in a partition in order to work properly: 
- `reduce` - needs all data to roll over the results 
- `count` - needs all data to count number of data
- `collect` - to gather all data into a concrete Python object
- `max`/ `min` -  need all data to iteratively find the max/ min of all data

![[3_hidden/_images/Pasted image 20230107172628.png]]

Whenever Action function is being called, ==Spark's RDD is no longer lazily evaluated and all the prior [[RDD Transformation|Transformations]]are computed== and concretised the data. The results of an Action is usually reported back to the Driver (e.g., `collect`) or write it to a external storage. 

---
## `reduce` 

The most use of RDD Actions is by implementing a `reduce` function, which has the following functional signature: `function(reduce_rdd, iter_rdd) -> rdd`. 
The `reduce_rdd` is the RDD output generated by the function and carry over to the next iteration and `iter_rdd` is the iterator of RDD.


```python
from pyspark.sql import Row

# `reduce` are a iterative function applies to all of the RDD
def reduce_sum(reduced_rdd: Row, input_rdd: Row) -> Row:
    """Compute the sum of `id` by reducing
    """
    print(f"reduce - {reduced_rdd}, input - {input_rdd}")
    sum_ = reduced_rdd.id + input_rdd.id
    return Row(id=sum_, name='sum')
    
# reduced results
rdds.reduce(reduce_sum)
```
![[3_hidden/_images/Pasted image 20230107170810.png]]

---
## ℹ️  Resources
- [[Apache Spark The Definitive Guide#^chapter-12]]
- [[🔖 learn-spark|Current Location]] ([zpln nb](https://github.com/tobytoyin/learn-spark/blob/main/pyspark/rdd-basics_2HPFJR37Q.zpln))