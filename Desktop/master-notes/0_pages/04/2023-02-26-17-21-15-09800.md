---
id: EBDF3601-D14E-4D49-9D93-778EDDE1D916
creation-date: 2022-12-24T16:36:55 
type: concept
alias: ['Spark Cluster', "cluster mode", "client mode", "cluster manager", "executor", "driver", "cluster"]
tags:  
- distributed-computing  
- technology/spark 
---

# Spark Cluster 

## Cluster vs Spark Cluster

Spark is a piece of Software that uses the concept of [[0_pages/03/2023-02-26-17-09-32-97200|Cluster]] as foundation. Essentially a Cluster is actual physical machines/ Nodes that forms a "**Cluster of Machines**", whereas a *Spark Cluster* is a "**Cluster of Processes**" that is running within a physical cluster: 

![[1_catalog/14/2022-12-24-16-36#^ca5523]] ^8fc455


---
## Components of a Spark Cluster

There are 3 major components that Spark uses/ communicates within a Cluster to do the job done.

> [!tip]- Spark Responsibility
> It is important to understand that Spark itself is setup and deploy a Cluster. But rather Spark "make uses of" a Cluster to run it software. 
> Similarly, when Spark runs on a local machine, it doesn't setup the OS but instead make uses of the OS. In both cases, Spark is just communicate with machine(s) to get resources for computation.

![[1_catalog/14/2022-12-24-16-36#^abd7a4]] ^74ffa5

- ***Cluster Manager*** ^76d11d
	- *Cluster Manager* is an external application service that ==maintains the Cluster and resources (Nodes)==
	- Spark is a software process that deployed in a running Cluster, the Cluster Manager assigns/ removes resources (Nodes) for Spark's process (Driver/ Executor)
- ***Spark Driver Process*** ^8776b7
	- *Spark Driver Process* is a ==process that controls the overall Spark execution== (i.e., application orchestration on what tasks to run; how to allocate tasks to Nodes)
	- It is a process that needs communicate with the Cluster Manager to interact with a Cluster: 
		- assign tasks to Worker Nodes
		- track states of task (success, failure, retries etc)
- ***Spark Executor Process*** ^03d370
	- *Spark Executor Process* is a process that is solely responsible for doing computation based on the tasks assigned by Spark Driver
	- Once the tasks are completed, it ==returns the results and states back to the Spark Driver==

---
## ℹ️ Resources
- [[Apache Spark The Definitive Guide#^chapter-15]]