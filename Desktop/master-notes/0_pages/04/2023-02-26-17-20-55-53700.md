---
id: 2B8F4486-392C-416E-8793-CFCF7672C050
creation-date: 2023-01-07T12:12:23 
type: walkthrough
alias: ['RDD Partitioners', "Partitioner", "repartition", "partitioning"]
tags: [ technology/spark/pyspark, technology/spark, example, optimisation ]
---
# RDD Partitioners 

## Controlling Partitions
![[0_pages/03/2023-02-26-17-12-01-86700#^partition-strategy-principle]]

*RDD Partitioner* is the RDD implementation of this principle:  ^66974b
1. [[0_pages/03/2023-02-26-17-19-01-95400|extract/ generate key from RDD data]]
2. create a new index based on data's key (*Partitioning Function*)
3. physically allocate (*shuffle*) data into new partitions 

---
## Hash Index Partitioning

Spark has two build in method to conduct [[0_pages/03/2023-02-26-15-38-47-17000|Hash Key Partitioning]] without having us to code up our own partitioning function: 
- `coalesce(int)` - only to reduce the number of partitions (*collapses partitions*)
- `repartition(int)` - declaratively change the number of partitions

Just like Hash Index Partitioning, these Partitioner distributes data mostly random and we have no control over which data would assign to which partitions. 

---
## Custom Partitioning Example

There are several reasons why we might want to implement our own custom partitioning strategy. 

If we know beforehand that certain partitioning would benefits performance, we can implement our own `partitionBy` strategy to bring data together. This is useful when we know that ==some computations are tightly coupled==. 

For example, consider the below dataframe: 
![[3_hidden/_images/Pasted image 20230107203536.png]]

The default RDD spreads the data into different partitions: 
![[3_hidden/_images/Pasted image 20230107203745.png]]

We could potentially improve the performance by bring similar data closer into the same partition. In this scenario, we can repartition the data based on `country`. We follow the [[#^66974b|above 3 steps]] to repartition the RDD:

```python 
# partition the data by country
def custom_country_partition(key): 
    if key == 'uk': 
        return 0
    if key == 'us': 
        return 1
    if key == 'eu': 
        return 2
    return 3

country_rdd = df.rdd\
	.keyBy(lambda rdd: rdd['country'])\
    .partitionBy(4, partitionFunc=custom_country_partition)
```
![[3_hidden/_images/Pasted image 20230107204350.png]]

What the above does is that: 
- `keyBy` - [[0_pages/03/2023-02-26-17-19-01-95400|extracts keys from the RDD data]]
- `partitionBy` - physically partition the data based on a partition function
- partition function - control which new index to generate based on `country` to assign into the partition of the equal index

---
## ‚ÑπÔ∏è¬† Resources
- [[Apache Spark The Definitive Guide#^chapter-13]]
- [[üîñ learn-spark|Current Location]] ([zpln nb](https://github.com/tobytoyin/learn-spark/tree/main/pyspark))